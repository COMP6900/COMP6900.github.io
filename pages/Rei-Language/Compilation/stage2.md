---
layout: page
title: Stage 2
parent: Compilation
grand_parent: Rei
nav_order: 2
---

# Syntax Analysis

This is the second major part of a compiler 'frontend' that actually tries to build meaning from tokens and attributes
Based on ordering, tree structuring, etc

## Overview

The lexical analyser sends tokens to the parser, which forms a parse tree. When the entire sequence has been processed, so should the parse tree. Assume the output of a parser is always a parse tree like structure

- the parse tree is useful since we can use it to form a more generalised, lower level IR of the source code. This IR can expand on stuff to make everything explicit as possible and be generalised enough to do analysis on, i.e. optimisation steps
- each step of the frontend usually has access to the global symbol table formed in the lexical analyser

## Grammars

Type 1 -> Universal \
Type 2 -> Top-Down \
Type 3 -> Bottom-Up

In most cases, a bottom-up grammar is better since the parsers for them are more efficient

For universal parsing, we can use CYK or Earley's algorithm. These are quite inefficient, so much so that they arent really practical in real life situations

If we want a really efficient parser (TD or BU), they usually only work for certain classes of grammars. Namely LL and LR

- these grammars are expressive enough to describe most of the syntax/nesting in most modern languages. So they dont seem like a bad choice
- LL can be implemented by hand
- LR are ormally generated using automated tools

### Representative Grammars

Constructs that begin with `while`, `if`, and other "structured" keywords are usually easier to parse. But expressions that involve identifiers and operators are more of a problem

- operators should have a precedence, like `()` before everything else, `*` and `/` before `+` and `-`, etc.

We can specify a list of 'rules' for expressions:

```
E -> E + T | T
T -> T * F | F
F -> (E) | id
```

- these rules describe the associativity and precedence of terms. `(expression)` expressions are prioritised. `*` expressions are next in line. And `+` are the least priority
- note how the priority is built from `F` and `T` then `E`. If we look at it from top-bottom, it is recursive. This means the grammar is an LR grammar. LR grammars can be parsed bottom-up

For a top-down grammar, we dont use recursion. Instead we specify extra rules:

```
E -> T E'
E' -> + T E' | epsilon
T -> F T'
T' -> *F T' | epsilon
F -> (E) | id 
```

- note how the grammars are all 'circular'. `F` relies on `E` and vice-versa

## Errors in Syntax

Compilers cant expect the input to be always correct. Humans make mistakes. Many mistakes. The compiler should even expect some common mistakes (e.g. rust) and suggest ways to correct them. But at least they should be able to spot a syntax error and throw some kind of meaningful error message about the problem

- its a good idea to plan how to tackle syntax errors. Prob better if you know exactly how your language looks, edge case testing, and common errors

### Types of Errors

Lexical -> misspellings of identifiers, keywords, operators

Syntactic -> misplaced semicolons, extra/missing braces and keywords

Semantic -> mismatch between operators and operands. Prob a bit harder to analyse off the bat. If you are returning an `Int` but the return type is actually `none`, then you have a syntax error

- one way is to not specify a return type sometimes

Logical -> incorrect reasoning, e.g. using the `=` operator when they actually wanted `==`. The end program may compile OK but the logic might not work as expected. These errors seem hardest to detect as they arent much to do with program validity but intention accuracy. Tests and etc should help and also a parser that checks for common things like this and raises a warning

Syntactic errors are usually easily detectable with usual parsing methods. LL and LR methods detect errors as soon as possible, if the stream of tokens do not match the grammar, then something is wrong

- usually it makes sense to just panic, print out the line and surrounding context where an error was detected, and quit

Goals of an error handler:

- report presence of errors clearly and accurately
- recover from each error quickly enough to detect subsequent errors
- minimum overhead when processing correct programs

## Context Free Grammars

The formal definition of a CFG:

- a grammar that contains terminals, nonterminals, start symbols, productions
- terminals = symbols from which strings are formed
- non-terminals = syntactic variables that represent sets of strings. A statement or expression is a nonterminal. Imposes a hierarchical structure on the language -> parse tree
- start symbol = a way to distinguish a certain nonterminal. The set of strings the nonterminal represents is the langauge generated by the grammar (lexeme). "Productions" for start symbols should be listed first
- productions = a way to specify the manner in which the terminals and nonterminals can be combined to form strings

A production consists of:

- a non terminal "head" or "left" of the production. This defines some of the strings represented by the head
- the symbol `->`
- a "body" or "right" of the production. Consists of zero or more terminals and non terminals. Describes one way in which strings of the nonterminal at the head can be constructed

### Notations

Terminals are either:

- lowercase letters like `a`
- operators like `+`
- punctuation like `,`
- digits `0, 1 ...`
- bolded strings like `id`, `if`. Each of these represents a single terminal symbol

Nonterminals are:

- uppercase letters like `A`
- `S`, which is the start symbol for a production
- lowercase and italicised strings like `expr`, `stmt`

Note, uppercase letters may also be used to represent more generic programming constructions like expressions, terms and factors

Also:

- Later uppercase `X, Y, Z` are usually used to represent grammar symbols like nonterminals and terminals. So they are metasymbol
- Later lowercase `x, y, z` usually represent possibly empty strings of terminals
- Lowercase greek letters like alpha are used to represent strings of grammar symbols. E.g. `A -> alpha` which means head -> body
- Productions like `A -> alpha_1` can be written with a common head `A -> alpha_1, alpha_2 ... alpha_k`
- Usually the head of the first production is the start symbol, regardless of whether it is `S` or something else

Example:

```
E -> E + T | E - T | T
T -> T * F | T / F | F
F -> (E) | id
```

- Nonterminals: E, T, F. There are 3 productions. All nonterminals are recursive and have circular dependencies
- E is the start symbol
- In the 1st production, we have 5 nonterminals. There are 2 terminals (operators `+` and `-`)
- In the 2nd production, we also have 5 nonterminals. Tere are also 2 terminals (operators `*` and `/`)
- In the 3rd production, we have a nonterminal `E` and a terminal `id`
- Note how the terminals can be directly parsed while the nonterminals need more context to be directly parsed into lexemes

### Derivations

We can treat productions as rewriting rules.

- We start at the start symbol of a production. Then we apply a bunch of rewrite steps
- Each rewrite step replaces a nonterminal with the body of one of it's productions
- For bottom up parsing, we use something called "rightmost derivations"

Example grammar:

```
E -> E + E | E * E | -E | (E) | id
```

- `-E` means if `E` is an expression, then `-E` is also an expression. Kind of like negation of the expression
- `(E)` is applied to replace any instance of `E` by `(E)` like `E * E => (E) * E`. It seems weird but is quite useful

This means with the above grammar for any expression `E`, we can do:

```
E => -E => -(E) => -(id)
```

- we have derived `-(id)` from `E`. Meaning a string `-(id)` is an instance of an expression
- this is important for seeing whether a grammar is context free and parseable by bottom-up parsing algorithms

We use `=>*` to mean 'derives in zero or more steps' and `=>+` to mean `derives in one or more steps`.

- if `S =>* alpha`, that means `alpha` is a sequential form of the grammar `G`. Given `S` is the start symbol for `G`

### Context Free Derivation

A sequential form may contain both terminals + nonterminals, or could be empty (epsilon). A sentence of `G` is a sequential form with no nonterminals, so it is completely self-contained/direct.

- the language generated by a grammar is its full set of sentences. So a string of terminals `omega` is in L(G) iff `omega` is a sentence of G. I.e. `S =>* w`
- this also means `w` can be derived from the grammar through as many steps to derive as possible. It just has to get there eventually. We can see how formal validation of an entire source code file could work

Hence: A language that can be generated by a grammar is `context-free`.

- if two grammars generate the same language (result in the same set of sentences), then they are equivalent/identical

With a context-free grammar, we can take as long as needed to derive something like `-(id+id)` from the start symbol `E`:

```
E => -E => -(E) => -(E + E) => -(id + E) => -(id + id)
```

### Leftmost derivations

We always choose the leftmost nonterminal in each sentence. The example above is leftmost since we are always replacing the leftmost symbol (nonterminal).

### Rightmost derivations

We always choose the rightmost nonterminal. They are also called 'canonical derivations' as they are often used for bottom up parsers.

## Parse Trees

A graphical view of a 'derivation'. It filters out the order which productions are applied to replace nonterminals.

- each internal node represents the application of a production to a sequence of chars we want to validate
- an interior node labeled `A` is a nonterminal
- nodes that are children of `A` are labeled from left -> right by symbols of the body of the production of `A`
- for any derivation `alpha_1 => alpha_2 => ... alpha_n` we can construct parse tree to yield each `alpha_i`

![](/assets/img/rei/parse_tree2.png)

- note how the 2nd and 3rd levels branch into 3 subnodes
- KEY: a non-ambiguous grammar produces only one parse tree for a valid sentence

### Ambiguous Grammars

Produces more than one leftmost or more than one rightmost derivation for the same sentence.
